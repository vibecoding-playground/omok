{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa02c83",
   "metadata": {},
   "source": [
    "# Omok (Ïò§Î™©) AI - Colab Training (GPU)\n",
    "\n",
    "AlphaZero Ïä§ÌÉÄÏùº Í∞ïÌôîÌïôÏäµÏúºÎ°ú 9x9 Ïò§Î™© AIÎ•º ÌïôÏäµÌï©ÎãàÎã§.\n",
    "\n",
    "**ÌäπÏßï:**\n",
    "- Google DriveÏóê Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô Ï†ÄÏû•\n",
    "- ÏÑ∏ÏÖò Ï¢ÖÎ£å ÌõÑÏóêÎèÑ Ïù¥Ïñ¥ÏÑú ÌïôÏäµ Í∞ÄÎä•\n",
    "\n",
    "**ÏÇ¨Ïö©Î≤ï:**\n",
    "1. Îü∞ÌÉÄÏûÑ > Îü∞ÌÉÄÏûÑ Ïú†Ìòï Î≥ÄÍ≤Ω > **GPU** ÏÑ†ÌÉù\n",
    "2. Î™®Îì† ÏÖÄ Ïã§Ìñâ (Í∏∞Ï°¥ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏûàÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú Ïù¥Ïñ¥ÏÑú ÌïôÏäµ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3202a44",
   "metadata": {},
   "source": [
    "## 0. Google Drive ÎßàÏö¥Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/omok'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"torch\", \"numpy\", \"tqdm\", \"matplotlib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317be54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import IntEnum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device ÏÑ§Ï†ï - Colab GPU ÏÇ¨Ïö©\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc091",
   "metadata": {},
   "source": [
    "## 1. Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b202578",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Player(IntEnum):\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    EMPTY = 0\n",
    "\n",
    "\n",
    "class Board:\n",
    "    \"\"\"9x9 Omok board with AlphaZero-compatible state representation.\"\"\"\n",
    "    SIZE = 9\n",
    "    WIN_LENGTH = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._board = np.zeros((self.SIZE, self.SIZE), dtype=np.int8)\n",
    "        self._current_player = Player.BLACK\n",
    "        self._last_move: Optional[Tuple[int, int]] = None\n",
    "        self._move_count = 0\n",
    "        self._winner: Optional[Player] = None\n",
    "        self._game_over = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get board state as (2, 9, 9) tensor for neural network.\"\"\"\n",
    "        state = np.zeros((2, self.SIZE, self.SIZE), dtype=np.float32)\n",
    "        state[0] = (self._board == self._current_player).astype(np.float32)\n",
    "        state[1] = (self._board == -self._current_player).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self) -> np.ndarray:\n",
    "        return (self._board == Player.EMPTY).flatten()\n",
    "\n",
    "    @property\n",
    "    def current_player(self) -> Player:\n",
    "        return self._current_player\n",
    "\n",
    "    @property\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self._game_over\n",
    "\n",
    "    @property\n",
    "    def winner(self) -> Optional[Player]:\n",
    "        return self._winner\n",
    "\n",
    "    @property\n",
    "    def move_count(self) -> int:\n",
    "        return self._move_count\n",
    "\n",
    "    def play(self, action: int) -> Tuple[np.ndarray, float, bool]:\n",
    "        if self._game_over:\n",
    "            raise ValueError(\"Game is already over\")\n",
    "\n",
    "        row, col = divmod(action, self.SIZE)\n",
    "        if not (0 <= row < self.SIZE and 0 <= col < self.SIZE):\n",
    "            raise ValueError(f\"Invalid position: ({row}, {col})\")\n",
    "        if self._board[row, col] != Player.EMPTY:\n",
    "            raise ValueError(f\"Position ({row}, {col}) is already occupied\")\n",
    "\n",
    "        self._board[row, col] = self._current_player\n",
    "        self._last_move = (row, col)\n",
    "        self._move_count += 1\n",
    "\n",
    "        if self._check_win(row, col):\n",
    "            self._winner = self._current_player\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 1.0, True\n",
    "\n",
    "        if self._move_count >= self.SIZE * self.SIZE:\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 0.0, True\n",
    "\n",
    "        self._current_player = Player(-self._current_player)\n",
    "        return self.get_state(), 0.0, False\n",
    "\n",
    "    def _check_win(self, row: int, col: int) -> bool:\n",
    "        player = self._board[row, col]\n",
    "        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "\n",
    "        for dr, dc in directions:\n",
    "            count = 1\n",
    "            for sign in [1, -1]:\n",
    "                r, c = row + sign * dr, col + sign * dc\n",
    "                while 0 <= r < self.SIZE and 0 <= c < self.SIZE and self._board[r, c] == player:\n",
    "                    count += 1\n",
    "                    r, c = r + sign * dr, c + sign * dc\n",
    "            if count >= self.WIN_LENGTH:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def copy(self) -> \"Board\":\n",
    "        new_board = Board.__new__(Board)\n",
    "        new_board._board = self._board.copy()\n",
    "        new_board._current_player = self._current_player\n",
    "        new_board._last_move = self._last_move\n",
    "        new_board._move_count = self._move_count\n",
    "        new_board._winner = self._winner\n",
    "        new_board._game_over = self._game_over\n",
    "        return new_board\n",
    "\n",
    "\n",
    "# Quick test\n",
    "board = Board()\n",
    "print(f\"Board size: {Board.SIZE}x{Board.SIZE}\")\n",
    "print(f\"State shape: {board.get_state().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a529d2",
   "metadata": {},
   "source": [
    "## 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7f8d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + residual)\n",
    "\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"AlphaZero-style Policy-Value Network.\"\"\"\n",
    "\n",
    "    def __init__(self, board_size: int = 9, in_channels: int = 2,\n",
    "                 num_filters: int = 128, num_res_blocks: int = 4):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.action_size = board_size * board_size\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, num_filters, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(*[ResBlock(num_filters) for _ in range(num_res_blocks)])\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * board_size * board_size, self.action_size),\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(board_size * board_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.conv_block(x)\n",
    "        x = self.res_blocks(x)\n",
    "        policy = F.log_softmax(self.policy_head(x), dim=1)\n",
    "        value = self.value_head(x)\n",
    "        return policy, value\n",
    "\n",
    "    def predict(self, state: torch.Tensor, valid_moves: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0)\n",
    "            log_policy, value = self(state)\n",
    "            policy = torch.exp(log_policy).squeeze(0)\n",
    "            policy = policy * valid_moves.float()\n",
    "            policy_sum = policy.sum()\n",
    "            if policy_sum > 0:\n",
    "                policy = policy / policy_sum\n",
    "            else:\n",
    "                policy = valid_moves.float() / valid_moves.sum()\n",
    "            return policy, value.item()\n",
    "\n",
    "\n",
    "model = PolicyValueNet().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736ce18",
   "metadata": {},
   "source": [
    "## 3. MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf777d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MCTSConfig:\n",
    "    num_simulations: int = 200\n",
    "    c_puct: float = 1.5\n",
    "    dirichlet_alpha: float = 0.3\n",
    "    dirichlet_epsilon: float = 0.25\n",
    "    temperature: float = 1.0\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior: float):\n",
    "        self.prior = prior\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.children: Dict[int, \"Node\"] = {}\n",
    "        self.is_expanded = False\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
    "\n",
    "    def ucb_score(self, parent_visit_count: int, c_puct: float) -> float:\n",
    "        exploration = c_puct * self.prior * math.sqrt(parent_visit_count) / (1 + self.visit_count)\n",
    "        return self.value + exploration\n",
    "\n",
    "    def select_child(self, c_puct: float) -> Tuple[int, \"Node\"]:\n",
    "        best_score, best_action, best_child = -float(\"inf\"), -1, None\n",
    "        for action, child in self.children.items():\n",
    "            score = child.ucb_score(self.visit_count, c_puct)\n",
    "            if score > best_score:\n",
    "                best_score, best_action, best_child = score, action, child\n",
    "        return best_action, best_child\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, model: PolicyValueNet, config: MCTSConfig, device: torch.device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "    def search(self, board: Board, add_noise: bool = True) -> np.ndarray:\n",
    "        self.root = Node(prior=0.0)\n",
    "        self._expand(self.root, board)\n",
    "\n",
    "        if add_noise:\n",
    "            self._add_dirichlet_noise(self.root, board.get_valid_moves())\n",
    "\n",
    "        for _ in range(self.config.num_simulations):\n",
    "            node = self.root\n",
    "            scratch_board = board.copy()\n",
    "            search_path = [node]\n",
    "\n",
    "            while node.is_expanded and not scratch_board.is_game_over:\n",
    "                action, node = node.select_child(self.config.c_puct)\n",
    "                scratch_board.play(action)\n",
    "                search_path.append(node)\n",
    "\n",
    "            if scratch_board.is_game_over:\n",
    "                value = 0.0 if scratch_board.winner is None else -1.0\n",
    "            else:\n",
    "                value = self._expand(node, scratch_board)\n",
    "\n",
    "            self._backup(search_path, value)\n",
    "\n",
    "        return self._get_action_probs(board.get_valid_moves())\n",
    "\n",
    "    def _expand(self, node: Node, board: Board) -> float:\n",
    "        state = torch.from_numpy(board.get_state()).to(self.device)\n",
    "        valid_moves = torch.from_numpy(board.get_valid_moves()).to(self.device)\n",
    "        policy, value = self.model.predict(state, valid_moves)\n",
    "        policy = policy.cpu().numpy()\n",
    "\n",
    "        for action in range(len(policy)):\n",
    "            if valid_moves[action]:\n",
    "                node.children[action] = Node(prior=policy[action])\n",
    "        node.is_expanded = True\n",
    "        return value\n",
    "\n",
    "    def _add_dirichlet_noise(self, node: Node, valid_moves: np.ndarray):\n",
    "        valid_actions = np.where(valid_moves)[0]\n",
    "        noise = np.random.dirichlet([self.config.dirichlet_alpha] * len(valid_actions))\n",
    "        for i, action in enumerate(valid_actions):\n",
    "            if action in node.children:\n",
    "                node.children[action].prior = (\n",
    "                    (1 - self.config.dirichlet_epsilon) * node.children[action].prior\n",
    "                    + self.config.dirichlet_epsilon * noise[i]\n",
    "                )\n",
    "\n",
    "    def _backup(self, search_path: List[Node], value: float):\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            value = -value\n",
    "\n",
    "    def _get_action_probs(self, valid_moves: np.ndarray) -> np.ndarray:\n",
    "        visit_counts = np.zeros(81, dtype=np.float32)\n",
    "        for action, child in self.root.children.items():\n",
    "            visit_counts[action] = child.visit_count\n",
    "\n",
    "        if self.config.temperature == 0:\n",
    "            probs = np.zeros_like(visit_counts)\n",
    "            probs[np.argmax(visit_counts)] = 1.0\n",
    "        else:\n",
    "            visit_counts = visit_counts ** (1 / self.config.temperature)\n",
    "            probs = visit_counts / (visit_counts.sum() + 1e-8)\n",
    "        return probs\n",
    "\n",
    "    def select_action(self, probs: np.ndarray, deterministic: bool = False) -> int:\n",
    "        return int(np.argmax(probs)) if deterministic else int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "\n",
    "print(\"MCTS initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c89f82",
   "metadata": {},
   "source": [
    "## 4. Self-Play & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4af4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameRecord:\n",
    "    states: List[np.ndarray] = field(default_factory=list)\n",
    "    policies: List[np.ndarray] = field(default_factory=list)\n",
    "    players: List[Player] = field(default_factory=list)\n",
    "    winner: Player = None\n",
    "\n",
    "    def add_move(self, state: np.ndarray, policy: np.ndarray, player: Player):\n",
    "        self.states.append(state)\n",
    "        self.policies.append(policy)\n",
    "        self.players.append(player)\n",
    "\n",
    "    def get_training_samples(self) -> List[Tuple[np.ndarray, np.ndarray, float]]:\n",
    "        samples = []\n",
    "        for state, policy, player in zip(self.states, self.policies, self.players):\n",
    "            if self.winner is None:\n",
    "                value = 0.0\n",
    "            elif self.winner == player:\n",
    "                value = 1.0\n",
    "            else:\n",
    "                value = -1.0\n",
    "            samples.append((state, policy, value))\n",
    "        return samples\n",
    "\n",
    "\n",
    "class SelfPlayWorker:\n",
    "    def __init__(self, model: PolicyValueNet, mcts_config: MCTSConfig, device: torch.device):\n",
    "        self.model = model\n",
    "        self.mcts_config = mcts_config\n",
    "        self.device = device\n",
    "\n",
    "    def play_game(self, temperature_threshold: int = 15) -> GameRecord:\n",
    "        board = Board()\n",
    "        record = GameRecord()\n",
    "        mcts = MCTS(self.model, self.mcts_config, self.device)\n",
    "\n",
    "        while not board.is_game_over:\n",
    "            mcts.config.temperature = 1.0 if board.move_count < temperature_threshold else 0.1\n",
    "            state = board.get_state()\n",
    "            policy = mcts.search(board, add_noise=True)\n",
    "            record.add_move(state, policy, board.current_player)\n",
    "            action = mcts.select_action(policy, deterministic=False)\n",
    "            board.play(action)\n",
    "\n",
    "        record.winner = board.winner\n",
    "        return record\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.states, self.policies, self.values = [], [], []\n",
    "\n",
    "    def add(self, states: np.ndarray, policies: np.ndarray, values: np.ndarray):\n",
    "        for s, p, v in zip(states, policies, values):\n",
    "            if len(self.states) >= self.max_size:\n",
    "                self.states.pop(0)\n",
    "                self.policies.pop(0)\n",
    "                self.values.pop(0)\n",
    "            self.states.append(s)\n",
    "            self.policies.append(p)\n",
    "            self.values.append(v)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        indices = np.random.choice(len(self.states), size=batch_size, replace=False)\n",
    "        return (\n",
    "            np.array([self.states[i] for i in indices]),\n",
    "            np.array([self.policies[i] for i in indices]),\n",
    "            np.array([self.values[i] for i in indices]),\n",
    "        )\n",
    "\n",
    "    def get_all(self) -> Tuple[List, List, List]:\n",
    "        return self.states, self.policies, self.values\n",
    "\n",
    "    def load(self, states, policies, values):\n",
    "        self.states = list(states)\n",
    "        self.policies = list(policies)\n",
    "        self.values = list(values)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "print(\"Self-play system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34030b42",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759687ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters - GPU ÏµúÏ†ÅÌôî\n",
    "NUM_ITERATIONS = 50          # ÌïôÏäµ Î∞òÎ≥µ ÌöüÏàò\n",
    "GAMES_PER_ITERATION = 50     # Ïù¥ÌÑ∞Î†àÏù¥ÏÖòÎãπ ÏûêÍ∞ÄÎåÄÍµ≠ Ïàò\n",
    "MCTS_SIMULATIONS = 200       # MCTS ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌöüÏàò\n",
    "BATCH_SIZE = 128             # Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à (GPU Î©îÎ™®Î¶¨ ÌôúÏö©)\n",
    "EPOCHS_PER_ITERATION = 5     # Ïù¥ÌÑ∞Î†àÏù¥ÏÖòÎãπ ÌïôÏäµ ÏóêÌè≠\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_BUFFER_SIZE = 50000\n",
    "\n",
    "# Checkpoint paths\n",
    "CHECKPOINT_PATH = os.path.join(SAVE_DIR, \"checkpoint.pt\")\n",
    "BUFFER_PATH = os.path.join(SAVE_DIR, \"replay_buffer.pkl\")\n",
    "HISTORY_PATH = os.path.join(SAVE_DIR, \"history.pkl\")\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games/iter: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  MCTS sims: {MCTS_SIMULATIONS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065a0c4",
   "metadata": {},
   "source": [
    "## 6. Checkpoint Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef2889",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, iteration, history, buffer):\n",
    "    \"\"\"Save training state to Google Drive.\"\"\"\n",
    "    # Save model & optimizer\n",
    "    torch.save({\n",
    "        \"iteration\": iteration,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    }, CHECKPOINT_PATH)\n",
    "\n",
    "    # Save replay buffer\n",
    "    states, policies, values = buffer.get_all()\n",
    "    with open(BUFFER_PATH, \"wb\") as f:\n",
    "        pickle.dump({\"states\": states, \"policies\": policies, \"values\": values}, f)\n",
    "\n",
    "    # Save history\n",
    "    with open(HISTORY_PATH, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    print(f\"  üíæ Checkpoint saved (iteration {iteration})\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, buffer):\n",
    "    \"\"\"Load training state from Google Drive if exists.\"\"\"\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"No checkpoint found. Starting fresh.\")\n",
    "        return 0, {\"policy_loss\": [], \"value_loss\": [], \"game_length\": []}\n",
    "\n",
    "    # Load model & optimizer\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    iteration = checkpoint[\"iteration\"]\n",
    "\n",
    "    # Load replay buffer\n",
    "    if os.path.exists(BUFFER_PATH):\n",
    "        with open(BUFFER_PATH, \"rb\") as f:\n",
    "            buf_data = pickle.load(f)\n",
    "            buffer.load(buf_data[\"states\"], buf_data[\"policies\"], buf_data[\"values\"])\n",
    "\n",
    "    # Load history\n",
    "    history = {\"policy_loss\": [], \"value_loss\": [], \"game_length\": []}\n",
    "    if os.path.exists(HISTORY_PATH):\n",
    "        with open(HISTORY_PATH, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "\n",
    "    print(f\"‚úÖ Checkpoint loaded! Resuming from iteration {iteration}\")\n",
    "    print(f\"   Buffer size: {len(buffer)}\")\n",
    "    return iteration, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39131ed2",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef725df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = PolicyValueNet().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    buffer = ReplayBuffer(MAX_BUFFER_SIZE)\n",
    "    mcts_config = MCTSConfig(num_simulations=MCTS_SIMULATIONS)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    start_iteration, history = load_checkpoint(model, optimizer, scheduler, buffer)\n",
    "\n",
    "    if start_iteration >= NUM_ITERATIONS:\n",
    "        print(f\"Already completed {NUM_ITERATIONS} iterations!\")\n",
    "        return model, history\n",
    "\n",
    "    for iteration in range(start_iteration + 1, NUM_ITERATIONS + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Self-play\n",
    "        model.eval()\n",
    "        worker = SelfPlayWorker(model, mcts_config, device)\n",
    "        games = []\n",
    "\n",
    "        for _ in tqdm(range(GAMES_PER_ITERATION), desc=\"Self-play\"):\n",
    "            game = worker.play_game()\n",
    "            games.append(game)\n",
    "\n",
    "        # Collect samples\n",
    "        all_samples = []\n",
    "        for game in games:\n",
    "            all_samples.extend(game.get_training_samples())\n",
    "\n",
    "        states = np.array([s[0] for s in all_samples], dtype=np.float32)\n",
    "        policies = np.array([s[1] for s in all_samples], dtype=np.float32)\n",
    "        values = np.array([s[2] for s in all_samples], dtype=np.float32)\n",
    "        buffer.add(states, policies, values)\n",
    "\n",
    "        avg_length = np.mean([len(g.states) for g in games])\n",
    "        history[\"game_length\"].append(avg_length)\n",
    "        print(f\"Avg game length: {avg_length:.1f}, Buffer: {len(buffer)}\")\n",
    "\n",
    "        # Training\n",
    "        if len(buffer) < BATCH_SIZE:\n",
    "            save_checkpoint(model, optimizer, scheduler, iteration, history, buffer)\n",
    "            continue\n",
    "\n",
    "        model.train()\n",
    "        total_policy_loss, total_value_loss, num_batches = 0.0, 0.0, 0\n",
    "\n",
    "        for epoch in range(EPOCHS_PER_ITERATION):\n",
    "            for _ in range(len(buffer) // BATCH_SIZE):\n",
    "                batch_states, batch_policies, batch_values = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "                states_t = torch.from_numpy(batch_states).to(device)\n",
    "                policies_t = torch.from_numpy(batch_policies).to(device)\n",
    "                values_t = torch.from_numpy(batch_values).to(device).unsqueeze(1)\n",
    "\n",
    "                log_policy, value = model(states_t)\n",
    "                policy_loss = -torch.sum(policies_t * log_policy) / log_policy.shape[0]\n",
    "                value_loss = F.mse_loss(value, values_t)\n",
    "                loss = policy_loss + value_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_policy_loss = total_policy_loss / num_batches\n",
    "        avg_value_loss = total_value_loss / num_batches\n",
    "        history[\"policy_loss\"].append(avg_policy_loss)\n",
    "        history[\"value_loss\"].append(avg_value_loss)\n",
    "\n",
    "        print(f\"Policy loss: {avg_policy_loss:.4f}, Value loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after every iteration\n",
    "        save_checkpoint(model, optimizer, scheduler, iteration, history, buffer)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training...\")\n",
    "trained_model, history = train()\n",
    "print(\"\\nüéâ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(history[\"policy_loss\"]) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(history[\"policy_loss\"])\n",
    "    axes[0].set_title(\"Policy Loss\")\n",
    "    axes[0].set_xlabel(\"Iteration\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    axes[1].plot(history[\"value_loss\"])\n",
    "    axes[1].set_title(\"Value Loss\")\n",
    "    axes[1].set_xlabel(\"Iteration\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "\n",
    "    axes[2].plot(history[\"game_length\"])\n",
    "    axes[2].set_title(\"Average Game Length\")\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "    axes[2].set_ylabel(\"Moves\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"training_curves.png\"))\n",
    "    plt.show()\n",
    "    print(f\"Training curves saved to {SAVE_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5eaf3",
   "metadata": {},
   "source": [
    "## 8. Export Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22529d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Save final model for local use\n",
    "FINAL_MODEL_PATH = os.path.join(SAVE_DIR, \"omok_model.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": trained_model.state_dict(),\n",
    "    \"history\": history,\n",
    "}, FINAL_MODEL_PATH)\n",
    "print(f\"Final model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Download option\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(FINAL_MODEL_PATH)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d492a2b",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667596f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    \"\"\"Play a test game and show the result.\"\"\"\n",
    "    model.eval()\n",
    "    board = Board()\n",
    "    mcts_config = MCTSConfig(num_simulations=100, temperature=0.1)\n",
    "    mcts = MCTS(model, mcts_config, device)\n",
    "\n",
    "    moves = []\n",
    "    while not board.is_game_over:\n",
    "        probs = mcts.search(board, add_noise=False)\n",
    "        action = mcts.select_action(probs, deterministic=True)\n",
    "        row, col = divmod(action, 9)\n",
    "        moves.append((board.current_player, row, col))\n",
    "        board.play(action)\n",
    "\n",
    "    # Display final board\n",
    "    symbols = {Player.EMPTY: \"¬∑\", Player.BLACK: \"‚óè\", Player.WHITE: \"‚óã\"}\n",
    "    print(\"\\nTest game result:\")\n",
    "    print(\"  \" + \" \".join(str(i) for i in range(9)))\n",
    "    for r in range(9):\n",
    "        row_str = f\"{r} \"\n",
    "        for c in range(9):\n",
    "            row_str += symbols[Player(board._board[r, c])] + \" \"\n",
    "        print(row_str)\n",
    "\n",
    "    if board.winner:\n",
    "        winner = \"Black ‚óè\" if board.winner == Player.BLACK else \"White ‚óã\"\n",
    "        print(f\"\\nWinner: {winner} ({len(moves)} moves)\")\n",
    "    else:\n",
    "        print(f\"\\nDraw! ({len(moves)} moves)\")\n",
    "\n",
    "test_model(trained_model)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
