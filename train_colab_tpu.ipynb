{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c409185",
   "metadata": {},
   "source": [
    "# Omok (오목) AI - JAX/TPU Training\n",
    "\n",
    "AlphaZero 스타일 강화학습으로 9x9 오목 AI를 TPU에서 학습합니다.\n",
    "\n",
    "**사용법:**\n",
    "1. 런타임 > 런타임 유형 변경 > **TPU** 선택\n",
    "2. 모든 셀 실행\n",
    "3. 학습 완료 후 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Install JAX with TPU support\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"jax[tpu]\", \"-f\", \"https://storage.googleapis.com/jax-releases/libtpu_releases.html\"])\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"flax\", \"optax\", \"numpy\", \"tqdm\", \"matplotlib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0f90b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import functools\n",
    "from enum import IntEnum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap, pmap\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Check devices\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "print(f\"Local device count: {jax.local_device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5555377",
   "metadata": {},
   "source": [
    "## 1. Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e932857",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Player(IntEnum):\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    EMPTY = 0\n",
    "\n",
    "\n",
    "class Board:\n",
    "    \"\"\"9x9 Omok board - pure NumPy for compatibility.\"\"\"\n",
    "    SIZE = 9\n",
    "    WIN_LENGTH = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._board = np.zeros((self.SIZE, self.SIZE), dtype=np.int8)\n",
    "        self._current_player = Player.BLACK\n",
    "        self._last_move: Optional[Tuple[int, int]] = None\n",
    "        self._move_count = 0\n",
    "        self._winner: Optional[Player] = None\n",
    "        self._game_over = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get board state as (2, 9, 9) array.\"\"\"\n",
    "        state = np.zeros((2, self.SIZE, self.SIZE), dtype=np.float32)\n",
    "        state[0] = (self._board == self._current_player).astype(np.float32)\n",
    "        state[1] = (self._board == -self._current_player).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self) -> np.ndarray:\n",
    "        return (self._board == Player.EMPTY).flatten()\n",
    "\n",
    "    @property\n",
    "    def current_player(self) -> Player:\n",
    "        return self._current_player\n",
    "\n",
    "    @property\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self._game_over\n",
    "\n",
    "    @property\n",
    "    def winner(self) -> Optional[Player]:\n",
    "        return self._winner\n",
    "\n",
    "    @property\n",
    "    def move_count(self) -> int:\n",
    "        return self._move_count\n",
    "\n",
    "    def play(self, action: int) -> Tuple[np.ndarray, float, bool]:\n",
    "        if self._game_over:\n",
    "            raise ValueError(\"Game is already over\")\n",
    "\n",
    "        row, col = divmod(action, self.SIZE)\n",
    "        if not (0 <= row < self.SIZE and 0 <= col < self.SIZE):\n",
    "            raise ValueError(f\"Invalid position\")\n",
    "        if self._board[row, col] != Player.EMPTY:\n",
    "            raise ValueError(f\"Position occupied\")\n",
    "\n",
    "        self._board[row, col] = self._current_player\n",
    "        self._last_move = (row, col)\n",
    "        self._move_count += 1\n",
    "\n",
    "        if self._check_win(row, col):\n",
    "            self._winner = self._current_player\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 1.0, True\n",
    "\n",
    "        if self._move_count >= self.SIZE * self.SIZE:\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 0.0, True\n",
    "\n",
    "        self._current_player = Player(-self._current_player)\n",
    "        return self.get_state(), 0.0, False\n",
    "\n",
    "    def _check_win(self, row: int, col: int) -> bool:\n",
    "        player = self._board[row, col]\n",
    "        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "        for dr, dc in directions:\n",
    "            count = 1\n",
    "            for sign in [1, -1]:\n",
    "                r, c = row + sign * dr, col + sign * dc\n",
    "                while 0 <= r < self.SIZE and 0 <= c < self.SIZE and self._board[r, c] == player:\n",
    "                    count += 1\n",
    "                    r, c = r + sign * dr, c + sign * dc\n",
    "            if count >= self.WIN_LENGTH:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def copy(self) -> \"Board\":\n",
    "        new_board = Board.__new__(Board)\n",
    "        new_board._board = self._board.copy()\n",
    "        new_board._current_player = self._current_player\n",
    "        new_board._last_move = self._last_move\n",
    "        new_board._move_count = self._move_count\n",
    "        new_board._winner = self._winner\n",
    "        new_board._game_over = self._game_over\n",
    "        return new_board\n",
    "\n",
    "\n",
    "print(f\"Board size: {Board.SIZE}x{Board.SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04a8d1",
   "metadata": {},
   "source": [
    "## 2. Neural Network (Flax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f86d2a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with BatchNorm.\"\"\"\n",
    "    channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        residual = x\n",
    "        x = nn.Conv(self.channels, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(self.channels, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        return nn.relu(x + residual)\n",
    "\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"AlphaZero-style Policy-Value Network in Flax.\"\"\"\n",
    "    num_filters: int = 128\n",
    "    num_res_blocks: int = 4\n",
    "    board_size: int = 9\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        action_size = self.board_size * self.board_size\n",
    "\n",
    "        # Initial conv block\n",
    "        x = nn.Conv(self.num_filters, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # Residual tower\n",
    "        for _ in range(self.num_res_blocks):\n",
    "            x = ResBlock(self.num_filters)(x, train=train)\n",
    "\n",
    "        # Policy head\n",
    "        policy = nn.Conv(2, (1, 1), use_bias=False)(x)\n",
    "        policy = nn.BatchNorm(use_running_average=not train)(policy)\n",
    "        policy = nn.relu(policy)\n",
    "        policy = policy.reshape((policy.shape[0], -1))  # Flatten\n",
    "        policy = nn.Dense(action_size)(policy)\n",
    "        policy = nn.log_softmax(policy)\n",
    "\n",
    "        # Value head\n",
    "        value = nn.Conv(1, (1, 1), use_bias=False)(x)\n",
    "        value = nn.BatchNorm(use_running_average=not train)(value)\n",
    "        value = nn.relu(value)\n",
    "        value = value.reshape((value.shape[0], -1))  # Flatten\n",
    "        value = nn.Dense(64)(value)\n",
    "        value = nn.relu(value)\n",
    "        value = nn.Dense(1)(value)\n",
    "        value = nn.tanh(value)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = PolicyValueNet()\n",
    "rng = random.PRNGKey(42)\n",
    "\n",
    "# Create dummy input (batch, height, width, channels) - NHWC format for JAX\n",
    "dummy_input = jnp.ones((1, 9, 9, 2))\n",
    "variables = model.init(rng, dummy_input, train=False)\n",
    "\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(variables['params']))\n",
    "print(f\"Model parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546c4d8",
   "metadata": {},
   "source": [
    "## 3. MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab710c2b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MCTSConfig:\n",
    "    num_simulations: int = 200\n",
    "    c_puct: float = 1.5\n",
    "    dirichlet_alpha: float = 0.3\n",
    "    dirichlet_epsilon: float = 0.25\n",
    "    temperature: float = 1.0\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior: float):\n",
    "        self.prior = prior\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.children: Dict[int, \"Node\"] = {}\n",
    "        self.is_expanded = False\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
    "\n",
    "    def ucb_score(self, parent_visit_count: int, c_puct: float) -> float:\n",
    "        exploration = c_puct * self.prior * math.sqrt(parent_visit_count) / (1 + self.visit_count)\n",
    "        return self.value + exploration\n",
    "\n",
    "    def select_child(self, c_puct: float) -> Tuple[int, \"Node\"]:\n",
    "        best_score, best_action, best_child = -float(\"inf\"), -1, None\n",
    "        for action, child in self.children.items():\n",
    "            score = child.ucb_score(self.visit_count, c_puct)\n",
    "            if score > best_score:\n",
    "                best_score, best_action, best_child = score, action, child\n",
    "        return best_action, best_child\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, apply_fn, params, batch_stats, config: MCTSConfig):\n",
    "        self.apply_fn = apply_fn\n",
    "        self.params = params\n",
    "        self.batch_stats = batch_stats\n",
    "        self.config = config\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "        # JIT compile the predict function\n",
    "        @jit\n",
    "        def predict_fn(params, batch_stats, state):\n",
    "            # state: (2, 9, 9) -> (1, 9, 9, 2) NHWC\n",
    "            state = state.transpose(1, 2, 0)[None, ...]\n",
    "            variables = {'params': params, 'batch_stats': batch_stats}\n",
    "            (log_policy, value), _ = apply_fn(\n",
    "                variables, state, train=False, mutable=['batch_stats']\n",
    "            )\n",
    "            return jax.nn.softmax(log_policy[0]), value[0, 0]\n",
    "\n",
    "        self.predict_fn = predict_fn\n",
    "\n",
    "    def search(self, board: Board, add_noise: bool = True) -> np.ndarray:\n",
    "        self.root = Node(prior=0.0)\n",
    "        self._expand(self.root, board)\n",
    "\n",
    "        if add_noise:\n",
    "            self._add_dirichlet_noise(self.root, board.get_valid_moves())\n",
    "\n",
    "        for _ in range(self.config.num_simulations):\n",
    "            node = self.root\n",
    "            scratch_board = board.copy()\n",
    "            search_path = [node]\n",
    "\n",
    "            while node.is_expanded and not scratch_board.is_game_over:\n",
    "                action, node = node.select_child(self.config.c_puct)\n",
    "                scratch_board.play(action)\n",
    "                search_path.append(node)\n",
    "\n",
    "            if scratch_board.is_game_over:\n",
    "                value = 0.0 if scratch_board.winner is None else -1.0\n",
    "            else:\n",
    "                value = self._expand(node, scratch_board)\n",
    "\n",
    "            self._backup(search_path, value)\n",
    "\n",
    "        return self._get_action_probs(board.get_valid_moves())\n",
    "\n",
    "    def _expand(self, node: Node, board: Board) -> float:\n",
    "        state = jnp.array(board.get_state())\n",
    "        valid_moves = board.get_valid_moves()\n",
    "\n",
    "        policy, value = self.predict_fn(self.params, self.batch_stats, state)\n",
    "        policy = np.array(policy)\n",
    "\n",
    "        # Mask invalid moves\n",
    "        policy = policy * valid_moves\n",
    "        policy_sum = policy.sum()\n",
    "        if policy_sum > 0:\n",
    "            policy = policy / policy_sum\n",
    "        else:\n",
    "            policy = valid_moves / valid_moves.sum()\n",
    "\n",
    "        for action in range(len(policy)):\n",
    "            if valid_moves[action]:\n",
    "                node.children[action] = Node(prior=policy[action])\n",
    "        node.is_expanded = True\n",
    "        return float(value)\n",
    "\n",
    "    def _add_dirichlet_noise(self, node: Node, valid_moves: np.ndarray):\n",
    "        valid_actions = np.where(valid_moves)[0]\n",
    "        noise = np.random.dirichlet([self.config.dirichlet_alpha] * len(valid_actions))\n",
    "        for i, action in enumerate(valid_actions):\n",
    "            if action in node.children:\n",
    "                node.children[action].prior = (\n",
    "                    (1 - self.config.dirichlet_epsilon) * node.children[action].prior\n",
    "                    + self.config.dirichlet_epsilon * noise[i]\n",
    "                )\n",
    "\n",
    "    def _backup(self, search_path: List[Node], value: float):\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            value = -value\n",
    "\n",
    "    def _get_action_probs(self, valid_moves: np.ndarray) -> np.ndarray:\n",
    "        visit_counts = np.zeros(81, dtype=np.float32)\n",
    "        for action, child in self.root.children.items():\n",
    "            visit_counts[action] = child.visit_count\n",
    "\n",
    "        if self.config.temperature == 0:\n",
    "            probs = np.zeros_like(visit_counts)\n",
    "            probs[np.argmax(visit_counts)] = 1.0\n",
    "        else:\n",
    "            visit_counts = visit_counts ** (1 / self.config.temperature)\n",
    "            probs = visit_counts / (visit_counts.sum() + 1e-8)\n",
    "        return probs\n",
    "\n",
    "    def select_action(self, probs: np.ndarray, deterministic: bool = False) -> int:\n",
    "        return int(np.argmax(probs)) if deterministic else int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "\n",
    "print(\"MCTS ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d38052",
   "metadata": {},
   "source": [
    "## 4. Self-Play & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameRecord:\n",
    "    states: List[np.ndarray] = field(default_factory=list)\n",
    "    policies: List[np.ndarray] = field(default_factory=list)\n",
    "    players: List[Player] = field(default_factory=list)\n",
    "    winner: Player = None\n",
    "\n",
    "    def add_move(self, state: np.ndarray, policy: np.ndarray, player: Player):\n",
    "        self.states.append(state)\n",
    "        self.policies.append(policy)\n",
    "        self.players.append(player)\n",
    "\n",
    "    def get_training_samples(self) -> List[Tuple[np.ndarray, np.ndarray, float]]:\n",
    "        samples = []\n",
    "        for state, policy, player in zip(self.states, self.policies, self.players):\n",
    "            value = 0.0 if self.winner is None else (1.0 if self.winner == player else -1.0)\n",
    "            samples.append((state, policy, value))\n",
    "        return samples\n",
    "\n",
    "\n",
    "class SelfPlayWorker:\n",
    "    def __init__(self, apply_fn, params, batch_stats, mcts_config: MCTSConfig):\n",
    "        self.apply_fn = apply_fn\n",
    "        self.params = params\n",
    "        self.batch_stats = batch_stats\n",
    "        self.mcts_config = mcts_config\n",
    "\n",
    "    def play_game(self, temperature_threshold: int = 15) -> GameRecord:\n",
    "        board = Board()\n",
    "        record = GameRecord()\n",
    "        mcts = MCTS(self.apply_fn, self.params, self.batch_stats, self.mcts_config)\n",
    "\n",
    "        while not board.is_game_over:\n",
    "            mcts.config.temperature = 1.0 if board.move_count < temperature_threshold else 0.1\n",
    "            state = board.get_state()\n",
    "            policy = mcts.search(board, add_noise=True)\n",
    "            record.add_move(state, policy, board.current_player)\n",
    "            action = mcts.select_action(policy, deterministic=False)\n",
    "            board.play(action)\n",
    "\n",
    "        record.winner = board.winner\n",
    "        return record\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.states, self.policies, self.values = [], [], []\n",
    "\n",
    "    def add(self, states: np.ndarray, policies: np.ndarray, values: np.ndarray):\n",
    "        for s, p, v in zip(states, policies, values):\n",
    "            if len(self.states) >= self.max_size:\n",
    "                self.states.pop(0)\n",
    "                self.policies.pop(0)\n",
    "                self.values.pop(0)\n",
    "            self.states.append(s)\n",
    "            self.policies.append(p)\n",
    "            self.values.append(v)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        indices = np.random.choice(len(self.states), size=batch_size, replace=False)\n",
    "        return (\n",
    "            np.array([self.states[i] for i in indices]),\n",
    "            np.array([self.policies[i] for i in indices]),\n",
    "            np.array([self.values[i] for i in indices]),\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "print(\"Self-play system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404cae4",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb105d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Hyperparameters - TPU optimized\n",
    "NUM_ITERATIONS = 30\n",
    "GAMES_PER_ITERATION = 50\n",
    "MCTS_SIMULATIONS = 200\n",
    "BATCH_SIZE = 256  # Larger batch for TPU\n",
    "EPOCHS_PER_ITERATION = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_BUFFER_SIZE = 50000\n",
    "\n",
    "print(f\"Training config (TPU optimized):\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games/iter: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f25157",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ac453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    \"\"\"Initialize training state with model and optimizer.\"\"\"\n",
    "    dummy_input = jnp.ones((1, 9, 9, 2))\n",
    "    variables = model.init(rng, dummy_input, train=True)\n",
    "\n",
    "    # Optimizer with gradient clipping\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate, weight_decay=1e-4),\n",
    "    )\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=variables['params'],\n",
    "        tx=tx,\n",
    "        batch_stats=variables['batch_stats'],\n",
    "    )\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(state: TrainState, batch_states, batch_policies, batch_values):\n",
    "    \"\"\"Single training step - JIT compiled.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        (log_policy, value), updates = state.apply_fn(\n",
    "            variables, batch_states, train=True, mutable=['batch_stats']\n",
    "        )\n",
    "\n",
    "        # Policy loss: cross-entropy\n",
    "        policy_loss = -jnp.sum(batch_policies * log_policy) / log_policy.shape[0]\n",
    "\n",
    "        # Value loss: MSE\n",
    "        value_loss = jnp.mean((value.squeeze() - batch_values) ** 2)\n",
    "\n",
    "        total_loss = policy_loss + value_loss\n",
    "        return total_loss, (policy_loss, value_loss, updates)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (policy_loss, value_loss, updates)), grads = grad_fn(state.params)\n",
    "\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=updates['batch_stats'])\n",
    "\n",
    "    return state, policy_loss, value_loss\n",
    "\n",
    "\n",
    "def train():\n",
    "    rng = random.PRNGKey(42)\n",
    "    model = PolicyValueNet()\n",
    "    state = create_train_state(rng, model, LEARNING_RATE)\n",
    "\n",
    "    mcts_config = MCTSConfig(num_simulations=MCTS_SIMULATIONS)\n",
    "    buffer = ReplayBuffer(MAX_BUFFER_SIZE)\n",
    "\n",
    "    history = {\"policy_loss\": [], \"value_loss\": [], \"game_length\": []}\n",
    "\n",
    "    for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Self-play\n",
    "        worker = SelfPlayWorker(\n",
    "            model.apply, state.params, state.batch_stats, mcts_config\n",
    "        )\n",
    "        games = []\n",
    "\n",
    "        for _ in tqdm(range(GAMES_PER_ITERATION), desc=\"Self-play\"):\n",
    "            game = worker.play_game()\n",
    "            games.append(game)\n",
    "\n",
    "        # Collect samples\n",
    "        all_samples = []\n",
    "        for game in games:\n",
    "            all_samples.extend(game.get_training_samples())\n",
    "\n",
    "        states = np.array([s[0] for s in all_samples], dtype=np.float32)\n",
    "        policies = np.array([s[1] for s in all_samples], dtype=np.float32)\n",
    "        values = np.array([s[2] for s in all_samples], dtype=np.float32)\n",
    "        buffer.add(states, policies, values)\n",
    "\n",
    "        avg_length = np.mean([len(g.states) for g in games])\n",
    "        history[\"game_length\"].append(avg_length)\n",
    "        print(f\"Avg game length: {avg_length:.1f}, Buffer: {len(buffer)}\")\n",
    "\n",
    "        # Training\n",
    "        if len(buffer) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        total_policy_loss, total_value_loss, num_batches = 0.0, 0.0, 0\n",
    "\n",
    "        for epoch in range(EPOCHS_PER_ITERATION):\n",
    "            for _ in range(len(buffer) // BATCH_SIZE):\n",
    "                batch_states, batch_policies, batch_values = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "                # Convert to JAX arrays (NHWC format)\n",
    "                batch_states = jnp.array(batch_states.transpose(0, 2, 3, 1))\n",
    "                batch_policies = jnp.array(batch_policies)\n",
    "                batch_values = jnp.array(batch_values)\n",
    "\n",
    "                state, policy_loss, value_loss = train_step(\n",
    "                    state, batch_states, batch_policies, batch_values\n",
    "                )\n",
    "\n",
    "                total_policy_loss += float(policy_loss)\n",
    "                total_value_loss += float(value_loss)\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_policy_loss = total_policy_loss / num_batches\n",
    "        avg_value_loss = total_value_loss / num_batches\n",
    "        history[\"policy_loss\"].append(avg_policy_loss)\n",
    "        history[\"value_loss\"].append(avg_value_loss)\n",
    "\n",
    "        print(f\"Policy loss: {avg_policy_loss:.4f}, Value loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "    return state, history\n",
    "\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training on TPU...\")\n",
    "trained_state, history = train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history[\"policy_loss\"])\n",
    "axes[0].set_title(\"Policy Loss\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "axes[1].plot(history[\"value_loss\"])\n",
    "axes[1].set_title(\"Value Loss\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "axes[2].plot(history[\"game_length\"])\n",
    "axes[2].set_title(\"Avg Game Length\")\n",
    "axes[2].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f5527",
   "metadata": {},
   "source": [
    "## 7. Save & Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52140f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save as pickle (JAX-native format)\n",
    "model_data = {\n",
    "    \"params\": trained_state.params,\n",
    "    \"batch_stats\": trained_state.batch_stats,\n",
    "    \"history\": history,\n",
    "}\n",
    "\n",
    "with open(\"omok_model_jax.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved to omok_model_jax.pkl\")\n",
    "\n",
    "# Download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"omok_model_jax.pkl\")\n",
    "    print(\"Download started!\")\n",
    "except ImportError:\n",
    "    print(\"Not on Colab - model saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963024d9",
   "metadata": {},
   "source": [
    "## 8. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(state):\n",
    "    \"\"\"Play a test game with the trained model.\"\"\"\n",
    "    board = Board()\n",
    "    mcts_config = MCTSConfig(num_simulations=100, temperature=0.1)\n",
    "    mcts = MCTS(PolicyValueNet().apply, state.params, state.batch_stats, mcts_config)\n",
    "\n",
    "    while not board.is_game_over:\n",
    "        probs = mcts.search(board, add_noise=False)\n",
    "        action = mcts.select_action(probs, deterministic=True)\n",
    "        board.play(action)\n",
    "\n",
    "    # Display\n",
    "    symbols = {Player.EMPTY: \"·\", Player.BLACK: \"●\", Player.WHITE: \"○\"}\n",
    "    print(\"\\nTest game result:\")\n",
    "    print(\"  \" + \" \".join(str(i) for i in range(9)))\n",
    "    for r in range(9):\n",
    "        row_str = f\"{r} \"\n",
    "        for c in range(9):\n",
    "            row_str += symbols[Player(board._board[r, c])] + \" \"\n",
    "        print(row_str)\n",
    "\n",
    "    if board.winner:\n",
    "        print(f\"\\nWinner: {'Black' if board.winner == Player.BLACK else 'White'}\")\n",
    "    else:\n",
    "        print(\"\\nDraw!\")\n",
    "\n",
    "test_model(trained_state)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
