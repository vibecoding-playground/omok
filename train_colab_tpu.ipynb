{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5b587a",
   "metadata": {},
   "source": [
    "# Omok (Ïò§Î™©) AI - JAX/TPU Training\n",
    "\n",
    "AlphaZero Ïä§ÌÉÄÏùº Í∞ïÌôîÌïôÏäµÏúºÎ°ú 9x9 Ïò§Î™© AIÎ•º TPUÏóêÏÑú ÌïôÏäµÌï©ÎãàÎã§.\n",
    "\n",
    "**ÌäπÏßï:**\n",
    "- Google DriveÏóê Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏûêÎèô Ï†ÄÏû•\n",
    "- ÏÑ∏ÏÖò Ï¢ÖÎ£å ÌõÑÏóêÎèÑ Ïù¥Ïñ¥ÏÑú ÌïôÏäµ Í∞ÄÎä•\n",
    "\n",
    "**ÏÇ¨Ïö©Î≤ï:**\n",
    "1. Îü∞ÌÉÄÏûÑ > Îü∞ÌÉÄÏûÑ Ïú†Ìòï Î≥ÄÍ≤Ω > **TPU** ÏÑ†ÌÉù\n",
    "2. Î™®Îì† ÏÖÄ Ïã§Ìñâ (Í∏∞Ï°¥ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Í∞Ä ÏûàÏúºÎ©¥ ÏûêÎèôÏúºÎ°ú Ïù¥Ïñ¥ÏÑú ÌïôÏäµ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0efbb",
   "metadata": {},
   "source": [
    "## 0. Google Drive ÎßàÏö¥Ìä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/omok'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Install JAX with TPU support\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"jax[tpu]\", \"-f\", \"https://storage.googleapis.com/jax-releases/libtpu_releases.html\"])\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"flax\", \"optax\", \"numpy\", \"tqdm\", \"matplotlib\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20728",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from enum import IntEnum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, vmap, pmap\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Check devices\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Device count: {jax.device_count()}\")\n",
    "print(f\"Local device count: {jax.local_device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbc951",
   "metadata": {},
   "source": [
    "## 1. Game Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c822091",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Player(IntEnum):\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "    EMPTY = 0\n",
    "\n",
    "\n",
    "class Board:\n",
    "    \"\"\"9x9 Omok board - pure NumPy for compatibility.\"\"\"\n",
    "    SIZE = 9\n",
    "    WIN_LENGTH = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._board = np.zeros((self.SIZE, self.SIZE), dtype=np.int8)\n",
    "        self._current_player = Player.BLACK\n",
    "        self._last_move: Optional[Tuple[int, int]] = None\n",
    "        self._move_count = 0\n",
    "        self._winner: Optional[Player] = None\n",
    "        self._game_over = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get board state as (2, 9, 9) array.\"\"\"\n",
    "        state = np.zeros((2, self.SIZE, self.SIZE), dtype=np.float32)\n",
    "        state[0] = (self._board == self._current_player).astype(np.float32)\n",
    "        state[1] = (self._board == -self._current_player).astype(np.float32)\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self) -> np.ndarray:\n",
    "        return (self._board == Player.EMPTY).flatten()\n",
    "\n",
    "    @property\n",
    "    def current_player(self) -> Player:\n",
    "        return self._current_player\n",
    "\n",
    "    @property\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self._game_over\n",
    "\n",
    "    @property\n",
    "    def winner(self) -> Optional[Player]:\n",
    "        return self._winner\n",
    "\n",
    "    @property\n",
    "    def move_count(self) -> int:\n",
    "        return self._move_count\n",
    "\n",
    "    def play(self, action: int) -> Tuple[np.ndarray, float, bool]:\n",
    "        if self._game_over:\n",
    "            raise ValueError(\"Game is already over\")\n",
    "\n",
    "        row, col = divmod(action, self.SIZE)\n",
    "        if not (0 <= row < self.SIZE and 0 <= col < self.SIZE):\n",
    "            raise ValueError(f\"Invalid position\")\n",
    "        if self._board[row, col] != Player.EMPTY:\n",
    "            raise ValueError(f\"Position occupied\")\n",
    "\n",
    "        self._board[row, col] = self._current_player\n",
    "        self._last_move = (row, col)\n",
    "        self._move_count += 1\n",
    "\n",
    "        if self._check_win(row, col):\n",
    "            self._winner = self._current_player\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 1.0, True\n",
    "\n",
    "        if self._move_count >= self.SIZE * self.SIZE:\n",
    "            self._game_over = True\n",
    "            return self.get_state(), 0.0, True\n",
    "\n",
    "        self._current_player = Player(-self._current_player)\n",
    "        return self.get_state(), 0.0, False\n",
    "\n",
    "    def _check_win(self, row: int, col: int) -> bool:\n",
    "        player = self._board[row, col]\n",
    "        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]\n",
    "        for dr, dc in directions:\n",
    "            count = 1\n",
    "            for sign in [1, -1]:\n",
    "                r, c = row + sign * dr, col + sign * dc\n",
    "                while 0 <= r < self.SIZE and 0 <= c < self.SIZE and self._board[r, c] == player:\n",
    "                    count += 1\n",
    "                    r, c = r + sign * dr, c + sign * dc\n",
    "            if count >= self.WIN_LENGTH:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def copy(self) -> \"Board\":\n",
    "        new_board = Board.__new__(Board)\n",
    "        new_board._board = self._board.copy()\n",
    "        new_board._current_player = self._current_player\n",
    "        new_board._last_move = self._last_move\n",
    "        new_board._move_count = self._move_count\n",
    "        new_board._winner = self._winner\n",
    "        new_board._game_over = self._game_over\n",
    "        return new_board\n",
    "\n",
    "\n",
    "print(f\"Board size: {Board.SIZE}x{Board.SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a5d4b",
   "metadata": {},
   "source": [
    "## 2. Neural Network (Flax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f5261",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with BatchNorm.\"\"\"\n",
    "    channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        residual = x\n",
    "        x = nn.Conv(self.channels, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(self.channels, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        return nn.relu(x + residual)\n",
    "\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"AlphaZero-style Policy-Value Network in Flax.\"\"\"\n",
    "    num_filters: int = 128\n",
    "    num_res_blocks: int = 4\n",
    "    board_size: int = 9\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        action_size = self.board_size * self.board_size\n",
    "\n",
    "        # Initial conv block\n",
    "        x = nn.Conv(self.num_filters, (3, 3), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        # Residual tower\n",
    "        for _ in range(self.num_res_blocks):\n",
    "            x = ResBlock(self.num_filters)(x, train=train)\n",
    "\n",
    "        # Policy head\n",
    "        policy = nn.Conv(2, (1, 1), use_bias=False)(x)\n",
    "        policy = nn.BatchNorm(use_running_average=not train)(policy)\n",
    "        policy = nn.relu(policy)\n",
    "        policy = policy.reshape((policy.shape[0], -1))\n",
    "        policy = nn.Dense(action_size)(policy)\n",
    "        policy = nn.log_softmax(policy)\n",
    "\n",
    "        # Value head\n",
    "        value = nn.Conv(1, (1, 1), use_bias=False)(x)\n",
    "        value = nn.BatchNorm(use_running_average=not train)(value)\n",
    "        value = nn.relu(value)\n",
    "        value = value.reshape((value.shape[0], -1))\n",
    "        value = nn.Dense(64)(value)\n",
    "        value = nn.relu(value)\n",
    "        value = nn.Dense(1)(value)\n",
    "        value = nn.tanh(value)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = PolicyValueNet()\n",
    "rng = random.PRNGKey(42)\n",
    "dummy_input = jnp.ones((1, 9, 9, 2))\n",
    "variables = model.init(rng, dummy_input, train=False)\n",
    "\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(variables['params']))\n",
    "print(f\"Model parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68855b",
   "metadata": {},
   "source": [
    "## 3. MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f5f0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MCTSConfig:\n",
    "    num_simulations: int = 200\n",
    "    c_puct: float = 1.5\n",
    "    dirichlet_alpha: float = 0.3\n",
    "    dirichlet_epsilon: float = 0.25\n",
    "    temperature: float = 1.0\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior: float):\n",
    "        self.prior = prior\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.children: Dict[int, \"Node\"] = {}\n",
    "        self.is_expanded = False\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
    "\n",
    "    def ucb_score(self, parent_visit_count: int, c_puct: float) -> float:\n",
    "        exploration = c_puct * self.prior * math.sqrt(parent_visit_count) / (1 + self.visit_count)\n",
    "        return self.value + exploration\n",
    "\n",
    "    def select_child(self, c_puct: float) -> Tuple[int, \"Node\"]:\n",
    "        best_score, best_action, best_child = -float(\"inf\"), -1, None\n",
    "        for action, child in self.children.items():\n",
    "            score = child.ucb_score(self.visit_count, c_puct)\n",
    "            if score > best_score:\n",
    "                best_score, best_action, best_child = score, action, child\n",
    "        return best_action, best_child\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, apply_fn, params, batch_stats, config: MCTSConfig):\n",
    "        self.apply_fn = apply_fn\n",
    "        self.params = params\n",
    "        self.batch_stats = batch_stats\n",
    "        self.config = config\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "        @jit\n",
    "        def predict_fn(params, batch_stats, state):\n",
    "            state = state.transpose(1, 2, 0)[None, ...]\n",
    "            variables = {'params': params, 'batch_stats': batch_stats}\n",
    "            (log_policy, value), _ = apply_fn(\n",
    "                variables, state, train=False, mutable=['batch_stats']\n",
    "            )\n",
    "            return jax.nn.softmax(log_policy[0]), value[0, 0]\n",
    "\n",
    "        self.predict_fn = predict_fn\n",
    "\n",
    "    def search(self, board: Board, add_noise: bool = True) -> np.ndarray:\n",
    "        self.root = Node(prior=0.0)\n",
    "        self._expand(self.root, board)\n",
    "\n",
    "        if add_noise:\n",
    "            self._add_dirichlet_noise(self.root, board.get_valid_moves())\n",
    "\n",
    "        for _ in range(self.config.num_simulations):\n",
    "            node = self.root\n",
    "            scratch_board = board.copy()\n",
    "            search_path = [node]\n",
    "\n",
    "            while node.is_expanded and not scratch_board.is_game_over:\n",
    "                action, node = node.select_child(self.config.c_puct)\n",
    "                scratch_board.play(action)\n",
    "                search_path.append(node)\n",
    "\n",
    "            if scratch_board.is_game_over:\n",
    "                value = 0.0 if scratch_board.winner is None else -1.0\n",
    "            else:\n",
    "                value = self._expand(node, scratch_board)\n",
    "\n",
    "            self._backup(search_path, value)\n",
    "\n",
    "        return self._get_action_probs(board.get_valid_moves())\n",
    "\n",
    "    def _expand(self, node: Node, board: Board) -> float:\n",
    "        state = jnp.array(board.get_state())\n",
    "        valid_moves = board.get_valid_moves()\n",
    "\n",
    "        policy, value = self.predict_fn(self.params, self.batch_stats, state)\n",
    "        policy = np.array(policy)\n",
    "\n",
    "        policy = policy * valid_moves\n",
    "        policy_sum = policy.sum()\n",
    "        if policy_sum > 0:\n",
    "            policy = policy / policy_sum\n",
    "        else:\n",
    "            policy = valid_moves / valid_moves.sum()\n",
    "\n",
    "        for action in range(len(policy)):\n",
    "            if valid_moves[action]:\n",
    "                node.children[action] = Node(prior=policy[action])\n",
    "        node.is_expanded = True\n",
    "        return float(value)\n",
    "\n",
    "    def _add_dirichlet_noise(self, node: Node, valid_moves: np.ndarray):\n",
    "        valid_actions = np.where(valid_moves)[0]\n",
    "        noise = np.random.dirichlet([self.config.dirichlet_alpha] * len(valid_actions))\n",
    "        for i, action in enumerate(valid_actions):\n",
    "            if action in node.children:\n",
    "                node.children[action].prior = (\n",
    "                    (1 - self.config.dirichlet_epsilon) * node.children[action].prior\n",
    "                    + self.config.dirichlet_epsilon * noise[i]\n",
    "                )\n",
    "\n",
    "    def _backup(self, search_path: List[Node], value: float):\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            value = -value\n",
    "\n",
    "    def _get_action_probs(self, valid_moves: np.ndarray) -> np.ndarray:\n",
    "        visit_counts = np.zeros(81, dtype=np.float32)\n",
    "        for action, child in self.root.children.items():\n",
    "            visit_counts[action] = child.visit_count\n",
    "\n",
    "        if self.config.temperature == 0:\n",
    "            probs = np.zeros_like(visit_counts)\n",
    "            probs[np.argmax(visit_counts)] = 1.0\n",
    "        else:\n",
    "            visit_counts = visit_counts ** (1 / self.config.temperature)\n",
    "            probs = visit_counts / (visit_counts.sum() + 1e-8)\n",
    "        return probs\n",
    "\n",
    "    def select_action(self, probs: np.ndarray, deterministic: bool = False) -> int:\n",
    "        return int(np.argmax(probs)) if deterministic else int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "\n",
    "print(\"MCTS ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cca27b",
   "metadata": {},
   "source": [
    "## 4. Self-Play & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameRecord:\n",
    "    states: List[np.ndarray] = field(default_factory=list)\n",
    "    policies: List[np.ndarray] = field(default_factory=list)\n",
    "    players: List[Player] = field(default_factory=list)\n",
    "    winner: Player = None\n",
    "\n",
    "    def add_move(self, state: np.ndarray, policy: np.ndarray, player: Player):\n",
    "        self.states.append(state)\n",
    "        self.policies.append(policy)\n",
    "        self.players.append(player)\n",
    "\n",
    "    def get_training_samples(self) -> List[Tuple[np.ndarray, np.ndarray, float]]:\n",
    "        samples = []\n",
    "        for state, policy, player in zip(self.states, self.policies, self.players):\n",
    "            value = 0.0 if self.winner is None else (1.0 if self.winner == player else -1.0)\n",
    "            samples.append((state, policy, value))\n",
    "        return samples\n",
    "\n",
    "\n",
    "class SelfPlayWorker:\n",
    "    def __init__(self, apply_fn, params, batch_stats, mcts_config: MCTSConfig):\n",
    "        self.apply_fn = apply_fn\n",
    "        self.params = params\n",
    "        self.batch_stats = batch_stats\n",
    "        self.mcts_config = mcts_config\n",
    "\n",
    "    def play_game(self, temperature_threshold: int = 15) -> GameRecord:\n",
    "        board = Board()\n",
    "        record = GameRecord()\n",
    "        mcts = MCTS(self.apply_fn, self.params, self.batch_stats, self.mcts_config)\n",
    "\n",
    "        while not board.is_game_over:\n",
    "            mcts.config.temperature = 1.0 if board.move_count < temperature_threshold else 0.1\n",
    "            state = board.get_state()\n",
    "            policy = mcts.search(board, add_noise=True)\n",
    "            record.add_move(state, policy, board.current_player)\n",
    "            action = mcts.select_action(policy, deterministic=False)\n",
    "            board.play(action)\n",
    "\n",
    "        record.winner = board.winner\n",
    "        return record\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        self.max_size = max_size\n",
    "        self.states, self.policies, self.values = [], [], []\n",
    "\n",
    "    def add(self, states: np.ndarray, policies: np.ndarray, values: np.ndarray):\n",
    "        for s, p, v in zip(states, policies, values):\n",
    "            if len(self.states) >= self.max_size:\n",
    "                self.states.pop(0)\n",
    "                self.policies.pop(0)\n",
    "                self.values.pop(0)\n",
    "            self.states.append(s)\n",
    "            self.policies.append(p)\n",
    "            self.values.append(v)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        indices = np.random.choice(len(self.states), size=batch_size, replace=False)\n",
    "        return (\n",
    "            np.array([self.states[i] for i in indices]),\n",
    "            np.array([self.policies[i] for i in indices]),\n",
    "            np.array([self.values[i] for i in indices]),\n",
    "        )\n",
    "\n",
    "    def get_all(self) -> Tuple[List, List, List]:\n",
    "        return self.states, self.policies, self.values\n",
    "\n",
    "    def load(self, states, policies, values):\n",
    "        self.states = list(states)\n",
    "        self.policies = list(policies)\n",
    "        self.values = list(values)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "print(\"Self-play system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd3699",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15518922",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Hyperparameters - TPU optimized\n",
    "NUM_ITERATIONS = 50\n",
    "GAMES_PER_ITERATION = 50\n",
    "MCTS_SIMULATIONS = 200\n",
    "BATCH_SIZE = 256  # Larger batch for TPU\n",
    "EPOCHS_PER_ITERATION = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_BUFFER_SIZE = 50000\n",
    "\n",
    "# Checkpoint paths\n",
    "CHECKPOINT_PATH = os.path.join(SAVE_DIR, \"checkpoint_jax.pkl\")\n",
    "BUFFER_PATH = os.path.join(SAVE_DIR, \"replay_buffer_jax.pkl\")\n",
    "HISTORY_PATH = os.path.join(SAVE_DIR, \"history_jax.pkl\")\n",
    "\n",
    "print(f\"Training config (TPU optimized):\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games/iter: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24daaf38",
   "metadata": {},
   "source": [
    "## 6. Training State & Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7e463",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    \"\"\"Initialize training state with model and optimizer.\"\"\"\n",
    "    dummy_input = jnp.ones((1, 9, 9, 2))\n",
    "    variables = model.init(rng, dummy_input, train=True)\n",
    "\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate, weight_decay=1e-4),\n",
    "    )\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=variables['params'],\n",
    "        tx=tx,\n",
    "        batch_stats=variables['batch_stats'],\n",
    "    )\n",
    "\n",
    "\n",
    "def save_checkpoint(state, iteration, history, buffer):\n",
    "    \"\"\"Save training state to Google Drive.\"\"\"\n",
    "    # Save model state\n",
    "    with open(CHECKPOINT_PATH, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"iteration\": iteration,\n",
    "            \"params\": state.params,\n",
    "            \"batch_stats\": state.batch_stats,\n",
    "            \"opt_state\": state.opt_state,\n",
    "        }, f)\n",
    "\n",
    "    # Save replay buffer\n",
    "    states, policies, values = buffer.get_all()\n",
    "    with open(BUFFER_PATH, \"wb\") as f:\n",
    "        pickle.dump({\"states\": states, \"policies\": policies, \"values\": values}, f)\n",
    "\n",
    "    # Save history\n",
    "    with open(HISTORY_PATH, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    print(f\"  üíæ Checkpoint saved (iteration {iteration})\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, buffer):\n",
    "    \"\"\"Load training state from Google Drive if exists.\"\"\"\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(\"No checkpoint found. Starting fresh.\")\n",
    "        rng = random.PRNGKey(42)\n",
    "        state = create_train_state(rng, model, LEARNING_RATE)\n",
    "        return state, 0, {\"policy_loss\": [], \"value_loss\": [], \"game_length\": []}\n",
    "\n",
    "    # Load model state\n",
    "    with open(CHECKPOINT_PATH, \"rb\") as f:\n",
    "        ckpt = pickle.load(f)\n",
    "\n",
    "    # Recreate state with loaded params\n",
    "    rng = random.PRNGKey(42)\n",
    "    state = create_train_state(rng, model, LEARNING_RATE)\n",
    "    state = state.replace(\n",
    "        params=ckpt[\"params\"],\n",
    "        batch_stats=ckpt[\"batch_stats\"],\n",
    "        opt_state=ckpt[\"opt_state\"],\n",
    "    )\n",
    "    iteration = ckpt[\"iteration\"]\n",
    "\n",
    "    # Load replay buffer\n",
    "    if os.path.exists(BUFFER_PATH):\n",
    "        with open(BUFFER_PATH, \"rb\") as f:\n",
    "            buf_data = pickle.load(f)\n",
    "            buffer.load(buf_data[\"states\"], buf_data[\"policies\"], buf_data[\"values\"])\n",
    "\n",
    "    # Load history\n",
    "    history = {\"policy_loss\": [], \"value_loss\": [], \"game_length\": []}\n",
    "    if os.path.exists(HISTORY_PATH):\n",
    "        with open(HISTORY_PATH, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "\n",
    "    print(f\"‚úÖ Checkpoint loaded! Resuming from iteration {iteration}\")\n",
    "    print(f\"   Buffer size: {len(buffer)}\")\n",
    "    return state, iteration, history\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(state: TrainState, batch_states, batch_policies, batch_values):\n",
    "    \"\"\"Single training step - JIT compiled.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': state.batch_stats}\n",
    "        (log_policy, value), updates = state.apply_fn(\n",
    "            variables, batch_states, train=True, mutable=['batch_stats']\n",
    "        )\n",
    "\n",
    "        policy_loss = -jnp.sum(batch_policies * log_policy) / log_policy.shape[0]\n",
    "        value_loss = jnp.mean((value.squeeze() - batch_values) ** 2)\n",
    "        total_loss = policy_loss + value_loss\n",
    "        return total_loss, (policy_loss, value_loss, updates)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (policy_loss, value_loss, updates)), grads = grad_fn(state.params)\n",
    "\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=updates['batch_stats'])\n",
    "\n",
    "    return state, policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7b332",
   "metadata": {},
   "source": [
    "## 7. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d714bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = PolicyValueNet()\n",
    "    buffer = ReplayBuffer(MAX_BUFFER_SIZE)\n",
    "    mcts_config = MCTSConfig(num_simulations=MCTS_SIMULATIONS)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    state, start_iteration, history = load_checkpoint(model, buffer)\n",
    "\n",
    "    if start_iteration >= NUM_ITERATIONS:\n",
    "        print(f\"Already completed {NUM_ITERATIONS} iterations!\")\n",
    "        return state, history\n",
    "\n",
    "    for iteration in range(start_iteration + 1, NUM_ITERATIONS + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Self-play\n",
    "        worker = SelfPlayWorker(\n",
    "            model.apply, state.params, state.batch_stats, mcts_config\n",
    "        )\n",
    "        games = []\n",
    "\n",
    "        for _ in tqdm(range(GAMES_PER_ITERATION), desc=\"Self-play\"):\n",
    "            game = worker.play_game()\n",
    "            games.append(game)\n",
    "\n",
    "        # Collect samples\n",
    "        all_samples = []\n",
    "        for game in games:\n",
    "            all_samples.extend(game.get_training_samples())\n",
    "\n",
    "        states = np.array([s[0] for s in all_samples], dtype=np.float32)\n",
    "        policies = np.array([s[1] for s in all_samples], dtype=np.float32)\n",
    "        values = np.array([s[2] for s in all_samples], dtype=np.float32)\n",
    "        buffer.add(states, policies, values)\n",
    "\n",
    "        avg_length = np.mean([len(g.states) for g in games])\n",
    "        history[\"game_length\"].append(avg_length)\n",
    "        print(f\"Avg game length: {avg_length:.1f}, Buffer: {len(buffer)}\")\n",
    "\n",
    "        # Training\n",
    "        if len(buffer) < BATCH_SIZE:\n",
    "            save_checkpoint(state, iteration, history, buffer)\n",
    "            continue\n",
    "\n",
    "        total_policy_loss, total_value_loss, num_batches = 0.0, 0.0, 0\n",
    "\n",
    "        for epoch in range(EPOCHS_PER_ITERATION):\n",
    "            for _ in range(len(buffer) // BATCH_SIZE):\n",
    "                batch_states, batch_policies, batch_values = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "                # Convert to JAX arrays (NHWC format)\n",
    "                batch_states = jnp.array(batch_states.transpose(0, 2, 3, 1))\n",
    "                batch_policies = jnp.array(batch_policies)\n",
    "                batch_values = jnp.array(batch_values)\n",
    "\n",
    "                state, policy_loss, value_loss = train_step(\n",
    "                    state, batch_states, batch_policies, batch_values\n",
    "                )\n",
    "\n",
    "                total_policy_loss += float(policy_loss)\n",
    "                total_value_loss += float(value_loss)\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_policy_loss = total_policy_loss / num_batches\n",
    "        avg_value_loss = total_value_loss / num_batches\n",
    "        history[\"policy_loss\"].append(avg_policy_loss)\n",
    "        history[\"value_loss\"].append(avg_value_loss)\n",
    "\n",
    "        print(f\"Policy loss: {avg_policy_loss:.4f}, Value loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after every iteration\n",
    "        save_checkpoint(state, iteration, history, buffer)\n",
    "\n",
    "    return state, history\n",
    "\n",
    "\n",
    "# Run training\n",
    "print(\"Starting training on TPU...\")\n",
    "trained_state, history = train()\n",
    "print(\"\\nüéâ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ae72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "if len(history[\"policy_loss\"]) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(history[\"policy_loss\"])\n",
    "    axes[0].set_title(\"Policy Loss\")\n",
    "    axes[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "    axes[1].plot(history[\"value_loss\"])\n",
    "    axes[1].set_title(\"Value Loss\")\n",
    "    axes[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "    axes[2].plot(history[\"game_length\"])\n",
    "    axes[2].set_title(\"Avg Game Length\")\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"training_curves_jax.png\"))\n",
    "    plt.show()\n",
    "    print(f\"Training curves saved to {SAVE_DIR}/training_curves_jax.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8f80e",
   "metadata": {},
   "source": [
    "## 8. Export Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835e4d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "FINAL_MODEL_PATH = os.path.join(SAVE_DIR, \"omok_model_jax.pkl\")\n",
    "model_data = {\n",
    "    \"params\": trained_state.params,\n",
    "    \"batch_stats\": trained_state.batch_stats,\n",
    "    \"history\": history,\n",
    "}\n",
    "\n",
    "with open(FINAL_MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "print(f\"Final model saved to {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Download option\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(FINAL_MODEL_PATH)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b93e06",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ed263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(state):\n",
    "    \"\"\"Play a test game with the trained model.\"\"\"\n",
    "    board = Board()\n",
    "    mcts_config = MCTSConfig(num_simulations=100, temperature=0.1)\n",
    "    mcts = MCTS(PolicyValueNet().apply, state.params, state.batch_stats, mcts_config)\n",
    "\n",
    "    while not board.is_game_over:\n",
    "        probs = mcts.search(board, add_noise=False)\n",
    "        action = mcts.select_action(probs, deterministic=True)\n",
    "        board.play(action)\n",
    "\n",
    "    # Display\n",
    "    symbols = {Player.EMPTY: \"¬∑\", Player.BLACK: \"‚óè\", Player.WHITE: \"‚óã\"}\n",
    "    print(\"\\nTest game result:\")\n",
    "    print(\"  \" + \" \".join(str(i) for i in range(9)))\n",
    "    for r in range(9):\n",
    "        row_str = f\"{r} \"\n",
    "        for c in range(9):\n",
    "            row_str += symbols[Player(board._board[r, c])] + \" \"\n",
    "        print(row_str)\n",
    "\n",
    "    if board.winner:\n",
    "        print(f\"\\nWinner: {'Black' if board.winner == Player.BLACK else 'White'}\")\n",
    "    else:\n",
    "        print(\"\\nDraw!\")\n",
    "\n",
    "test_model(trained_state)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
